# MIT 6.824

> MIT 6.824分布式系统的课程以及lab实现
>
> 本文章仅为记录，并不考虑别人的阅读体验，如果在阅读过程中觉得吃力或疑惑，你可以选择阅读原教程和原文章。
>
> 如果在阅读过程中发现错误或疑问，可以评论或私信，看到一定及时回复！

原课程地址：[MIT 6.824](https://pdos.csail.mit.edu/6.824/schedule.html)

B站中文翻译视频：[2020 MIT 6.824 分布式系统](https://www.bilibili.com/video/BV1R7411t71W/?share_source=copy_web&vd_source=e1e24c47f1c7ed2a5d313e7001d78147) 

翻译材料：[MIT 6.824](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-01-introduction/1.1-fen-bu-shi-xi-tong-de-qu-dong-li-he-tiao-zhan-drivens-and-challenges)

## MapReduce（论文）

> 原文：[MapReduce](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf)
>
> 中文译文（个人觉得比较好的）: [谷歌三大核心技术（二）Google MapReduce中文版](https://cloud.tencent.com/developer/article/1981246?shareByChannel=link&sharedUid=10535947#content)

### 1、 概要

通过 **map** 方法实现一个值对另一个值的映射，用  **reduce** 方法来合并所有的中间值，以获得最初的 *value* ；

### 2、 Introduction

分布式系统产生原因：越来越大量的数据输入（TB），但是需要在一个合理的时间范围内返回结果，一台机器并是解决的办法（成本，效率），更好的办法是并行且分布式的发布给很多台电脑来执行处理，然后返回正确的结果（光是这里已经能感受到问题很多，难度很大，包括但不限于多线程情况下的各种问题）

个人读下来的问题有：

- 如何并行执行？

- 如何分散数据？

- 如何让数据执行后结合并返回正确结果？

- 如果任何一台机器出现问题，其他机器怎么办或者如何得知其他机器的执行情况？

  这里个人猜测是：sockets，signal，可以借鉴TCP协议的可靠传输机制；



解决诸多复杂问题的办法：

应用MapReduce处理数据：

1. 利用map操作来获取一个键值对的集合，在所有具有相同key值的value使用reduce操作，来合并中间的数据，得到一个想要的结果的目的。
2. 因为Mapreduce的键值对特性，自带 **再次执行** 的功能，提供了初级的容灾方案。

### 3、 编程模型

Mapreduce原理： *利用一个输入键值对集合，来获取一个输出键值对集合* 。

用户自定义 **map** 函数，接收一个输入的键值对集合，MapReduce库把所有具有相同key值的中间value集合在一起然后传输给reduce；

用户自定义 **reduce** 函数，接收一个key值，和一个value值的集合，一般每次调用返回0或1个value，这样就解决了value集合太大无法放入内存的麻烦（每次迭代一个然后运算后返回？）

#### 01 单词统计伪代码示例

略。



#### 02 类型

尽管在前面例子的伪代码中使用了以字符串表示的输入输出值，但是在概念上，用户定义的Map和Reduce函数都有相关联的类型：

```c++
 map(k1,v1) -> list(k2,v2)
 reduce(k2,list(v2)) ->list(v2) 
```



#### 03 更多例子

略



### 4、 实现

![image-20230426224220737](https://gitee.com/moni_world/pic_bed/raw/master/img/image-20230426224220737.png)

​										                          																																（图1）

#### 01 概括

将数据分割成M个数据片段，并行执行在多台机器上，map调用，并将产生的中间key值分散到多个机器上执行；

步骤与**（图1）**一一对应（也许不会全写）：

1. 将数据分片为M个，然后用户程序在机群中创建大量副本；
2. 有一个特殊的 **master** 程序，其他都是worker，由master来分配任务，一共由M个map任务和R个reduce任务；（联想到了Reactor模式，用来处理大并发连接，可见这种模式的思想非常值得学习）；
3. worker从输入中读取相关片段，调用map函数，然后将输出的中间kv set缓存到内存中；
4. 然后缓存中的kv pair通过分区函数分成R个区域，然后周期性地写到磁盘上，缓存的位置返回给master，master再发送地址到Reduce worker；
5. Reduce worker接收到master发来的数据存储信息后，使用RPC从map worker读取信息，对key排序，让相同key的信息聚合在一起（数据量太大则从外部排序），**必须排序！** 
6. Reduce worker遍历排序后中间数据，对于每个唯一的key值，Reduce worker将所有中间value值的集合传递给reduce函数；
7. 所有任务执行完毕后，master唤醒用户函数，返回结果。



#### 02 Master数据结构

Master数据结构中存储了Map和Reduce任务的状态（空闲，工作，完成），以及Worker机器的状态（非空闲）。

像一个管道一样（Muduo中的Channel？）将Map执行完的数据存储，然后将存储位置传递给Reduce。



#### 03 容错

设计初衷是成百上千的机器一同工作，其中任何一台机器都有可能出错，所以需要容错机制。

##### Worker故障

Master周期性的ping每个worker（轮询的思想），如果一定时间没有回应，将该Worker标记为失效，所有失效的Worker完成的Map任务被重定义为初始的空闲状态（ **必须重新执行，因为map的结果在这台机器上， 已经无法访问** ），之后这些任务安排给其他的worker。同时所有正在运行的 map 和 reduce 任务重置为空闲状态，等待调度。

这个机制可以处理大规模的worker失效，即使很多机器失效，所需要做的工作也仅仅是将他们做的工作重新做一遍，然后再继续向下执行。



> 如果存在一种极端情况，绝大部分机器都已失效，此时从硬件层次考虑也许是更好的选择。因为软件的提升是有瓶颈的。



##### master失效

简单的解决办法是：周期性存储master状态， 如果master失效，就恢复到上一个 check point 的状态，但是这种办法是非常复杂的，因为只有一个master；我们的处理方式是：直接中止操作，然后根据这个状态重新执行MapReduce操作。



#### 04 失效处理

Map和Reduce都是输入确定性的函数，系统在任何情况下的输出和所有程序没有出现任何错误，顺序的执行产生的输出是一样的。

这依赖于M，R操作都是原子性提交，每个任务完成后，将输出存储到私有的临时文件中。每个Reduce任务生成一个这样的文件，而每个map任务则生成R个这样的文件，如果map完成，就信息传递给master，然后master存储（多次传输也只保留一次），然后将该临时文件的地址发送给reduce。

如果一个reduce在多个机器上执行，依赖于底层重命名的原子性，同时间只有一个Reduce任务产生的数据。

如果M，R是不确定性函数，有一个较弱的机制：

有M和Ri这样的任务，e（Ri）代表Ri已经提交的执行过程， e（Ri）分别读取M，就形成了较弱的失效处理。



#### 05 存储位置

网络带宽是一个匮乏的资源，所以我们尽量让各个任务在本地执行。实现方式是，通过把输入数据分割成需要多Blocks（64M），然后拷贝存储在多个机器上（一般3个），master在分配任务时，尽量分配到本地的任务，如果没有，就分配到就近的任务，这样让大部分的工作不占用网络带宽。（思想类似于多线程中，减少临界区的概念？学了多线程，发现其中的思想非常有意思，非常实用且简洁优雅。）



#### 06 任务粒度

为了提高负载均衡的能力，比较理想的状态是，M和R比worker机器多得多，每台机器都能执行大量不同的M，R操作，失效机器的M，R都可以在其他机器上执行。

但实际上M和R有一定的限制：

时间复杂度：O（M+R）次唤醒；

空间复杂度：O（M*R），这里影响较小，因为每个M+R差不多一个字节就足够。



R一般由用户指定，更倾向于指定M值，使得每个任务处理在16~64M之间，这样也正         *符合05存储位置* 那个地方的内存优化策略，然后把R设为我们想要的Worker机器数量小的倍数，一个常见的例子：

- M 200000, R 5000, worker 2000;



#### 07 备用任务

影响MapReduce总执行时间最多的一般是 “落伍者” ，一台机器出现故障导致速度非常慢，有一个通用的机制来解决问题：

当MapReduce接近完成的时候，master将任务调用给备用任务进程来执行剩下的，处于处理中的任务（没错，即使正在执行，也会调用给备用进程再重新执行），无论备用还是worker先处理完，都会标记为处理完，并加入master的数据结构。



### 5、技巧

#### 01 分区函数

缺省的分区函数采用hash， hash生成一个平均的分区，对于特定的，可以采取特定属性的hash。

#### 02 顺序保证

保证给定分区中的数据时按照 **key** 的增量顺序来处理，保证了输出文件的有序性。

个人思考：map读取出来后，保存在一个vector中是否可行呢，因为map的查找时间为O（1），然后用move函数来插入到一个vector中，每个key的value集合用一个vector来保存，但是问题在于怎么保存key呢？直接用数组名来保存是否可行？

#### 03 Combiner函数

Map函数在输出时可能存在大量的重复输出，“the 1”这样的键值对，如果让这种重复数据进入网络是非常浪费资源的。较好的处理方式是，在本地先进行一次合并，变成 {the， n}这样的键值对，然后存储到中间文件，再由Reduce来处理



#### 04 输入输出类型

通过一个Reader接口，来读取不同的数据类型（数据库，内存，文件）。



#### 05 副作用

看文章暂时没看出副作用是什么，但是提供了一个获得原子且幂等的结果的方法：

输出一个临时文件，然后对临时文件进行重命名操作。



#### 06 跳过损坏的记录

用户程序会导致MapReduce出现bug，任务直接crash，常规做法是解决bug然后重启MapReduce，但实际处理方式是，用信号的方式，发一个UDP包给master，master标记该条需要跳过（UDP包的原因：无连接，尽全力交付，很轻易地就能发送，即使丢失在下一次执行时也可以找到，但不管怎么样都存在丢失的可能）。



#### 07 本地执行

本地执行的版本专门为DEBUG而生，个人猜测是由多线程来实现的（又必须得提到Reactor模式了^_^）。



#### 08 状态信息

master使用一个嵌入式的HTTP服务器(Jetty)，监控各种信息：执行进度，中间数据，输入输出字节数，方便用户来跟踪错误信息和分配资源（不就是日志系统吗）。猜测这些信息是写在content-text里的。



#### 09 计数器

用户自定义一个计数器，每次执行Map和Reduce操作就会让计数器++（毫无疑问是原子性的）。

```c++
Counter* uppercase;
uppercase = GetCounter("uppercase");
map(String name, String contents):
for each word w in contents:
if (IsCapitalized(w)):
uppercase->Increment();
EmitIntermediate(w, "1");
```

周期性的传递给master，附加在ping包中（防失效的轮询机制），然后master进行汇总，

当整个MapReduce完成后，用户汇总。



### 6、性能，经验，相关工作

本节可能并不会全部记录，请参看原文。



### 7、总结

MapReduce封装了并行处理，容错处理（标记失效，重新分配），数据本地化优化（排序实现），负载均衡。

